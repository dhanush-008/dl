{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df135bb1-c61e-459c-aafb-efbe9bda0991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], d_loss: 0.6463, g_loss: 0.8963\n",
      "Epoch [2/100], d_loss: 0.6301, g_loss: 0.8505\n",
      "Epoch [3/100], d_loss: 0.3155, g_loss: 1.5057\n",
      "Epoch [4/100], d_loss: 1.3964, g_loss: 1.0284\n",
      "Epoch [5/100], d_loss: 2.3029, g_loss: 11.3853\n",
      "Epoch [6/100], d_loss: 2.3182, g_loss: 2.4406\n",
      "Epoch [7/100], d_loss: 1.5308, g_loss: 0.8348\n",
      "Epoch [8/100], d_loss: 1.0777, g_loss: 1.5070\n",
      "Epoch [9/100], d_loss: 0.8694, g_loss: 1.7199\n",
      "Epoch [10/100], d_loss: 1.8924, g_loss: 0.5584\n",
      "Epoch [11/100], d_loss: 1.9785, g_loss: 0.9841\n",
      "Epoch [12/100], d_loss: 0.2918, g_loss: 2.3639\n",
      "Epoch [13/100], d_loss: 1.4044, g_loss: 0.5039\n",
      "Epoch [14/100], d_loss: 0.6987, g_loss: 1.6821\n",
      "Epoch [15/100], d_loss: 1.4220, g_loss: 0.5217\n",
      "Epoch [16/100], d_loss: 1.5823, g_loss: 1.0236\n",
      "Epoch [17/100], d_loss: 1.3948, g_loss: 6.3302\n",
      "Epoch [18/100], d_loss: 1.9811, g_loss: 3.6895\n",
      "Epoch [19/100], d_loss: 0.8004, g_loss: 2.9090\n",
      "Epoch [20/100], d_loss: 0.6083, g_loss: 3.2473\n",
      "Epoch [21/100], d_loss: 0.8178, g_loss: 4.0133\n",
      "Epoch [22/100], d_loss: 0.8156, g_loss: 4.8234\n",
      "Epoch [23/100], d_loss: 0.4704, g_loss: 8.3002\n",
      "Epoch [24/100], d_loss: 0.4929, g_loss: 7.5385\n",
      "Epoch [25/100], d_loss: 0.7919, g_loss: 9.2127\n",
      "Epoch [26/100], d_loss: 0.7673, g_loss: 5.3142\n",
      "Epoch [27/100], d_loss: 0.1351, g_loss: 9.0538\n",
      "Epoch [28/100], d_loss: 0.3128, g_loss: 11.1814\n",
      "Epoch [29/100], d_loss: 0.3697, g_loss: 7.2607\n",
      "Epoch [30/100], d_loss: 0.6706, g_loss: 16.0219\n",
      "Epoch [31/100], d_loss: 0.2291, g_loss: 6.7603\n",
      "Epoch [32/100], d_loss: 0.0868, g_loss: 11.6718\n",
      "Epoch [33/100], d_loss: 0.1308, g_loss: 13.1480\n",
      "Epoch [34/100], d_loss: 0.1558, g_loss: 10.9810\n",
      "Epoch [35/100], d_loss: 0.1980, g_loss: 32.3355\n",
      "Epoch [36/100], d_loss: 0.3101, g_loss: 57.2872\n",
      "Epoch [37/100], d_loss: 0.0773, g_loss: 12.5017\n",
      "Epoch [38/100], d_loss: 0.0505, g_loss: 11.8258\n",
      "Epoch [39/100], d_loss: 0.1587, g_loss: 12.3417\n",
      "Epoch [40/100], d_loss: 0.0639, g_loss: 6.8476\n",
      "Epoch [41/100], d_loss: 0.0284, g_loss: 9.3994\n",
      "Epoch [42/100], d_loss: 0.0417, g_loss: 7.8525\n",
      "Epoch [43/100], d_loss: 0.1070, g_loss: 13.2625\n",
      "Epoch [44/100], d_loss: 0.0256, g_loss: 16.3762\n",
      "Epoch [45/100], d_loss: 0.0841, g_loss: 16.0820\n",
      "Epoch [46/100], d_loss: 0.0438, g_loss: 20.9634\n",
      "Epoch [47/100], d_loss: 0.1528, g_loss: 23.7439\n",
      "Epoch [48/100], d_loss: 0.4081, g_loss: 12.9501\n",
      "Epoch [49/100], d_loss: 0.2004, g_loss: 5.3767\n",
      "Epoch [50/100], d_loss: 0.4165, g_loss: 5.7584\n",
      "Epoch [51/100], d_loss: 0.2564, g_loss: 11.2693\n",
      "Epoch [52/100], d_loss: 1.0334, g_loss: 7.9820\n",
      "Epoch [53/100], d_loss: 0.7470, g_loss: 10.4169\n",
      "Epoch [54/100], d_loss: 0.7821, g_loss: 9.2560\n",
      "Epoch [55/100], d_loss: 0.1402, g_loss: 6.0169\n",
      "Epoch [56/100], d_loss: 0.1764, g_loss: 10.4624\n",
      "Epoch [57/100], d_loss: 0.2456, g_loss: 11.0026\n",
      "Epoch [58/100], d_loss: 0.1435, g_loss: 9.7170\n",
      "Epoch [59/100], d_loss: 0.1290, g_loss: 7.8942\n",
      "Epoch [60/100], d_loss: 0.0387, g_loss: 10.5325\n",
      "Epoch [61/100], d_loss: 0.0629, g_loss: 12.0915\n",
      "Epoch [62/100], d_loss: 0.0437, g_loss: 8.4544\n",
      "Epoch [63/100], d_loss: 0.1095, g_loss: 7.0287\n",
      "Epoch [64/100], d_loss: 0.0380, g_loss: 8.1595\n",
      "Epoch [65/100], d_loss: 0.0821, g_loss: 4.8608\n",
      "Epoch [66/100], d_loss: 0.1387, g_loss: 3.8840\n",
      "Epoch [67/100], d_loss: 0.1082, g_loss: 4.3863\n",
      "Epoch [68/100], d_loss: 0.3215, g_loss: 6.4406\n",
      "Epoch [69/100], d_loss: 0.4655, g_loss: 6.3575\n",
      "Epoch [70/100], d_loss: 0.3619, g_loss: 4.9303\n",
      "Epoch [71/100], d_loss: 0.2017, g_loss: 4.5850\n",
      "Epoch [72/100], d_loss: 0.4887, g_loss: 4.5288\n",
      "Epoch [73/100], d_loss: 0.1186, g_loss: 4.8098\n",
      "Epoch [74/100], d_loss: 0.1062, g_loss: 6.3078\n",
      "Epoch [75/100], d_loss: 0.1504, g_loss: 4.4201\n",
      "Epoch [76/100], d_loss: 0.0279, g_loss: 7.1000\n",
      "Epoch [77/100], d_loss: 0.1951, g_loss: 6.1630\n",
      "Epoch [78/100], d_loss: 0.1999, g_loss: 4.9158\n",
      "Epoch [79/100], d_loss: 0.1514, g_loss: 5.4863\n",
      "Epoch [80/100], d_loss: 0.4453, g_loss: 4.3590\n",
      "Epoch [81/100], d_loss: 0.4102, g_loss: 3.4029\n",
      "Epoch [82/100], d_loss: 0.1339, g_loss: 4.8531\n",
      "Epoch [83/100], d_loss: 0.3540, g_loss: 4.5191\n",
      "Epoch [84/100], d_loss: 0.3895, g_loss: 4.9878\n",
      "Epoch [85/100], d_loss: 0.1389, g_loss: 4.6624\n",
      "Epoch [86/100], d_loss: 0.0819, g_loss: 4.6252\n",
      "Epoch [87/100], d_loss: 0.5391, g_loss: 3.6931\n",
      "Epoch [88/100], d_loss: 0.8056, g_loss: 3.1514\n",
      "Epoch [89/100], d_loss: 0.2547, g_loss: 3.2876\n",
      "Epoch [90/100], d_loss: 0.4913, g_loss: 4.5771\n",
      "Epoch [91/100], d_loss: 0.2647, g_loss: 6.2181\n",
      "Epoch [92/100], d_loss: 0.2825, g_loss: 4.9049\n",
      "Epoch [93/100], d_loss: 0.2601, g_loss: 3.5575\n",
      "Epoch [94/100], d_loss: 0.1003, g_loss: 3.7095\n",
      "Epoch [95/100], d_loss: 0.4206, g_loss: 3.5793\n",
      "Epoch [96/100], d_loss: 0.3499, g_loss: 3.3418\n",
      "Epoch [97/100], d_loss: 0.4445, g_loss: 2.9542\n",
      "Epoch [98/100], d_loss: 0.2955, g_loss: 3.8613\n",
      "Epoch [99/100], d_loss: 0.1644, g_loss: 3.1616\n",
      "Epoch [100/100], d_loss: 0.1172, g_loss: 2.9815\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the Generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, text_embed_dim, img_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(noise_dim + text_embed_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, img_size * img_size * 3)  # 3 for RGB channels\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, noise, text_embedding):\n",
    "        x = torch.cat((noise, text_embedding), dim=1)  # Concatenate noise and text embedding\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.tanh(self.fc3(x))  # Output between -1 and 1 for images\n",
    "        x = x.view(-1, 3, img_size, img_size)  # Reshape to image dimensions\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the Discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_size, text_embed_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(img_size * img_size * 3 + text_embed_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, img, text_embedding):\n",
    "        img = img.view(img.size(0), -1)  # Flatten image\n",
    "        x = torch.cat((img, text_embedding), dim=1)  # Concatenate image and text embedding\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))  # Output between 0 and 1\n",
    "        return x\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "noise_dim = 100\n",
    "text_embed_dim = 128  # Text embedding dimension\n",
    "img_size = 64  # Image size (64x64)\n",
    "batch_size = 32\n",
    "lr = 0.0002\n",
    "num_epochs = 100\n",
    "\n",
    "# Create generator and discriminator\n",
    "G = Generator(noise_dim, text_embed_dim, img_size).to(device)\n",
    "D = Discriminator(img_size, text_embed_dim).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(G.parameters(), lr=lr)\n",
    "optimizer_D = optim.Adam(D.parameters(), lr=lr)\n",
    "\n",
    "# Function to generate random noise and text embeddings\n",
    "def generate_noise(batch_size, noise_dim):\n",
    "    return torch.randn(batch_size, noise_dim).to(device)\n",
    "\n",
    "def generate_text_embeddings(batch_size, text_embed_dim):\n",
    "    return torch.randn(batch_size, text_embed_dim).to(device)  # Fake text embeddings\n",
    "\n",
    "\n",
    "# Training the GAN\n",
    "for epoch in range(num_epochs):\n",
    "    for _ in range(batch_size):  # Adjusted to mimic mini-batches\n",
    "        # Generate fake and real data\n",
    "        real_imgs = torch.randn(batch_size, 3, img_size, img_size).to(device)  # Simulated real images\n",
    "        real_text_embeddings = generate_text_embeddings(batch_size, text_embed_dim)\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # Generate fake images\n",
    "        noise = generate_noise(batch_size, noise_dim)\n",
    "        fake_text_embeddings = generate_text_embeddings(batch_size, text_embed_dim)\n",
    "        fake_imgs = G(noise, fake_text_embeddings)\n",
    "\n",
    "        # Train Discriminator\n",
    "        outputs_real = D(real_imgs, real_text_embeddings)\n",
    "        d_loss_real = criterion(outputs_real, real_labels)\n",
    "\n",
    "        outputs_fake = D(fake_imgs.detach(), fake_text_embeddings)\n",
    "        d_loss_fake = criterion(outputs_fake, fake_labels)\n",
    "\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Train Generator\n",
    "        outputs_fake = D(fake_imgs, fake_text_embeddings)\n",
    "        g_loss = criterion(outputs_fake, real_labels)  # Generator tries to fool the discriminator\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    # Print progress\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')\n",
    "\n",
    "    # Save generated images every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        save_image(fake_imgs.data[:25], f'generated_images_{epoch + 1}.png', nrow=5, normalize=True)\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(G.state_dict(), 'generator.pth')\n",
    "torch.save(D.state_dict(), 'discriminator.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd994825-13d2-4cf2-bc45-cc007804b00d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
